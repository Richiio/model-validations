{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "\n",
    "DATAPATH = \"../data\"\n",
    "SMICOL = \"smiles\"\n",
    "INCHICOL = \"inchikey\"\n",
    "ACTCOL = \"activity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Model Training datasets\n",
    "\n",
    "First, we clean up the original files and add the InChiKey of the smiles if not available. We want to create a dataframe with three columns, smiles, inchikey and activity. We will store each dataset under data/model_datasets/{model_name}_processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:43:01] non-ring atom 10 marked aromatic\n",
      "[22:43:01] non-ring atom 12 marked aromatic\n",
      "[22:43:01] non-ring atom 10 marked aromatic\n",
      "[22:43:01] non-ring atom 14 marked aromatic\n",
      "[22:43:01] non-ring atom 10 marked aromatic\n",
      "[22:43:02] non-ring atom 10 marked aromatic\n",
      "[22:43:02] non-ring atom 10 marked aromatic\n",
      "[22:43:02] non-ring atom 21 marked aromatic\n",
      "[22:43:02] non-ring atom 10 marked aromatic\n",
      "[22:43:02] non-ring atom 10 marked aromatic\n",
      "[22:43:02] non-ring atom 12 marked aromatic\n",
      "[22:43:02] non-ring atom 12 marked aromatic\n",
      "[22:43:03] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:03] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:03] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:03] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 4, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 5, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 5, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 5, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 5, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 5, is greater than permitted\n",
      "[22:43:04] Explicit valence for atom # 0 N, 5, is greater than permitted\n",
      "[22:43:05] non-ring atom 11 marked aromatic\n",
      "[22:43:05] non-ring atom 13 marked aromatic\n",
      "[22:43:05] non-ring atom 10 marked aromatic\n",
      "[22:43:05] non-ring atom 10 marked aromatic\n",
      "[22:43:05] non-ring atom 11 marked aromatic\n",
      "[22:43:05] non-ring atom 10 marked aromatic\n",
      "[22:43:05] non-ring atom 10 marked aromatic\n",
      "[22:43:05] non-ring atom 9 marked aromatic\n",
      "[22:43:05] non-ring atom 21 marked aromatic\n",
      "[22:43:05] non-ring atom 10 marked aromatic\n",
      "[22:43:06] non-ring atom 10 marked aromatic\n",
      "[22:43:06] non-ring atom 13 marked aromatic\n",
      "[22:43:06] non-ring atom 10 marked aromatic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smiles eliminated:  49\n"
     ]
    }
   ],
   "source": [
    "#eos30gr\n",
    "\n",
    "train_data = pd.read_excel(os.path.join(DATAPATH, \"model_datasets\", \"eos30gr.xlsx\"), sheet_name=0)\n",
    "test_data = pd.read_excel(os.path.join(DATAPATH, \"model_datasets\", \"eos30gr.xlsx\"), sheet_name=1)\n",
    "valid_data = pd.read_excel(os.path.join(DATAPATH, \"model_datasets\", \"eos30gr.xlsx\"), sheet_name=2)\n",
    "eos30gr = pd.concat([train_data, test_data, valid_data])\n",
    "\n",
    "inchikeys = []\n",
    "for smi in eos30gr[\"Smiles\"]:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        inchikey = Chem.MolToInchiKey(mol)\n",
    "    else:\n",
    "        inchikey = None\n",
    "    inchikeys += [inchikey]\n",
    "\n",
    "eos30gr[INCHICOL] = inchikeys\n",
    "total_len = len(eos30gr)\n",
    "eos30gr.dropna(subset=[INCHICOL], inplace=True)\n",
    "print(\"Smiles eliminated: \", total_len-len(eos30gr))\n",
    "eos30gr.rename(columns={\"Smiles\":SMICOL, \"activity10\":ACTCOL}, inplace=True) #looking at the model, activity 10 was chosen for activity\n",
    "eos30gr = eos30gr[[SMICOL, INCHICOL, ACTCOL]]\n",
    "eos30gr.to_csv(os.path.join(DATAPATH, \"model_datasets\", \"eos30gr_processed.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3160574/3265443799.py:3: DtypeWarning: Columns (118,119,120,124,125,126,130,131,132,136,137,138,142,143,144,148,149,150,154,155,156,160,161,162,166,167,168,172,173,174,268,269,270,274,275,276,280,281,282,286,287,288,292,293,294,298,299,300,304,305,306,310,311,312,316,317,318,322,323,324,327,328,329,332,333,334,337,338,339,342,343,344,347,348,349,352,353,354,357,358,359,362,363,364,367,368,369,372,373,374,377,378,379,382,383,384,387,388,389,392,393,394,397,398,399,402,403,404,407,408,409,412,413,414,417,418,419,422,423,424,480,481,482,483,493,494,495,496,720) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\",\"eos2ta5\", \"train_validation_cardio_tox_data.csv\"))\n",
      "/tmp/ipykernel_3160574/3265443799.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eos2ta5[INCHICOL] = inchikeys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smiles eliminated:  0\n"
     ]
    }
   ],
   "source": [
    "#eos2ta5\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\",\"eos2ta5\", \"train_validation_cardio_tox_data.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\", \"eos2ta5\", 'external_test_set_neg.csv'))\n",
    "test_data2 = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\", \"eos2ta5\", 'external_test_set_new.csv'))\n",
    "test_data3 = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\", \"eos2ta5\", 'external_test_set_pos.csv'))\n",
    "\n",
    "eos2ta5 = pd.concat([train_data, test_data,test_data2, test_data3])\n",
    "\n",
    "inchikeys = []\n",
    "for smi in eos2ta5[\"smiles\"]:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        inchikey = Chem.MolToInchiKey(mol)\n",
    "    else:\n",
    "        inchikey = None\n",
    "    inchikeys += [inchikey]\n",
    "\n",
    "eos2ta5[INCHICOL] = inchikeys\n",
    "total_len = len(eos2ta5)\n",
    "eos2ta5.dropna(subset=[INCHICOL], inplace=True)\n",
    "print(\"Smiles eliminated: \", total_len-len(eos2ta5))\n",
    "eos2ta5.rename(columns={\"smiles\":SMICOL, \"ACTIVITY\":ACTCOL}, inplace=True) #looking at the model, activity 10 was chosen for activity\n",
    "eos2ta5 = eos2ta5[[SMICOL, INCHICOL, ACTCOL]]\n",
    "eos2ta5.to_csv(os.path.join(DATAPATH, \"model_datasets\", \"eos2ta5_processed.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smiles eliminated:  0\n"
     ]
    }
   ],
   "source": [
    "#eos4tcc\n",
    "finetuning_data = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\", \"eos4tcc\", \"finetuning_eos4tcc\", \"test_all.csv\"))\n",
    "finetuning_data2 = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\", \"eos4tcc\", \"finetuning_eos4tcc\", \"test_rev.csv\"))\n",
    "finetuning_data3 = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\", \"eos4tcc\", \"finetuning_eos4tcc\", \"training.csv\"))\n",
    "finetuning_data4 = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\",\"eos4tcc\", \"finetuning_eos4tcc\", \"val_all.csv\"))\n",
    "finetuning_data5 = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\",\"eos4tcc\", \"finetuning_eos4tcc\", \"val_rev.csv\"))\n",
    "eos4tcc = pd.concat([finetuning_data, finetuning_data2,finetuning_data3,finetuning_data4,finetuning_data5 ])\n",
    "\n",
    "inchikeys = []\n",
    "for smi in eos4tcc[\"smiles\"]:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        inchikey = Chem.MolToInchiKey(mol)\n",
    "    else:\n",
    "        inchikey = None\n",
    "    inchikeys += [inchikey]\n",
    "\n",
    "eos4tcc[INCHICOL] = inchikeys\n",
    "total_len = len(eos4tcc)\n",
    "eos4tcc.dropna(subset=[INCHICOL], inplace=True)\n",
    "print(\"Smiles eliminated: \", total_len-len(eos4tcc))\n",
    "eos4tcc.rename(columns={\"smiles\":SMICOL, \"label\":ACTCOL}, inplace=True) \n",
    "eos4tcc = eos4tcc[[SMICOL, INCHICOL, ACTCOL]]\n",
    "eos4tcc.to_csv(os.path.join(DATAPATH, \"model_datasets\", \"eos4tcc_processed.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smiles eliminated:  0\n"
     ]
    }
   ],
   "source": [
    "# eos30f3\n",
    "train_data = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\",\"eos30f3\", \"Cai_TableS3_fixed.csv\"))\n",
    "\n",
    "# Concatenating Data\n",
    "eos30f3 = pd.concat([train_data])\n",
    "\n",
    "# Generating InChiKeys\n",
    "inchikeys = []\n",
    "for smi in eos30f3[\"smiles\"]:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        inchikey = Chem.MolToInchiKey(mol)\n",
    "    else:\n",
    "        inchikey = None\n",
    "    inchikeys += [inchikey]\n",
    "\n",
    "# Adding InChiKeys to DataFrame and Dropping NaN Values\n",
    "eos30f3[INCHICOL] = inchikeys\n",
    "total_len = len(eos30f3)\n",
    "eos30f3.dropna(subset=[INCHICOL], inplace=True)\n",
    "print(\"Smiles eliminated: \", total_len - len(eos30f3))\n",
    "\n",
    "# Renaming Columns\n",
    "eos30f3.rename(columns={\"smiles\": SMICOL, \"X10\": ACTCOL}, inplace=True)\n",
    "\n",
    "# Selecting Columns\n",
    "eos30f3 = eos30f3[[SMICOL, INCHICOL, ACTCOL]]\n",
    "\n",
    "# Saving Processed Data\n",
    "eos30f3.to_csv(os.path.join(DATAPATH, \"model_datasets\", \"eos30f3_processed.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the datasets have been cleaned, we can compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos2ta5:\n",
      "activity\n",
      "0    6727\n",
      "1    6718\n",
      "Name: count, dtype: int64\n",
      "\n",
      "eos4tcc:\n",
      "activity\n",
      "1    9284\n",
      "0    6487\n",
      "Name: count, dtype: int64\n",
      "\n",
      "eos30f3:\n",
      "activity\n",
      "1    4355\n",
      "0    3534\n",
      "Name: count, dtype: int64\n",
      "\n",
      "eos30gr:\n",
      "activity\n",
      "1.0    4332\n",
      "0.0    3526\n",
      "Name: count, dtype: int64\n",
      "Number of repeated smiles in eos2ta5: 0\n",
      "Number of repeated smiles in eos4tcc: 1449\n",
      "Number of repeated smiles in eos30f3: 0\n",
      "Number of repeated smiles in eos30gr: 5120\n",
      "Number of repeated smiles between eos2ta5 and eos4tcc: 12028\n",
      "Number of repeated smiles between eos2ta5 and eos30f3: 42\n",
      "Number of repeated smiles between eos2ta5 and eos30gr: 5162\n",
      "Number of repeated smiles between eos4tcc and eos30f3: 1497\n",
      "Number of repeated smiles between eos4tcc and eos30gr: 6617\n",
      "Number of repeated smiles between eos30f3 and eos30gr: 12978\n"
     ]
    }
   ],
   "source": [
    "models = [\"eos2ta5\", \"eos4tcc\", \"eos30f3\", \"eos30gr\"]\n",
    "\n",
    "# load the datasets and make comparisons\n",
    "eos2ta5 = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\",\"eos2ta5_processed.csv\"))\n",
    "eos4tcc = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\",\"eos4tcc_processed.csv\"))\n",
    "eos30f3 = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\",\"eos30f3_processed.csv\"))\n",
    "eos30gr = pd.read_csv(os.path.join(DATAPATH, \"model_datasets\",\"eos30gr_processed.csv\"))\n",
    "\n",
    "# proportion of actives and inactives in each dataset\n",
    "print(\"eos2ta5:\")\n",
    "print(eos2ta5['activity'].value_counts())\n",
    "\n",
    "print(\"\\neos4tcc:\")\n",
    "print(eos4tcc['activity'].value_counts())\n",
    "\n",
    "print(\"\\neos30f3:\")\n",
    "print(eos30f3['activity'].value_counts())\n",
    "\n",
    "print(\"\\neos30gr:\")\n",
    "print(eos30gr['activity'].value_counts())\n",
    "\n",
    "# number of repeated smiles between models\n",
    "# Check repeated smiles within each dataset\n",
    "repeated_smiles_eos2ta5 = eos2ta5['smiles'].duplicated().sum()\n",
    "repeated_smiles_eos4tcc = eos4tcc['smiles'].duplicated().sum()\n",
    "repeated_smiles_eos30f3 = eos30f3['smiles'].duplicated().sum()\n",
    "repeated_smiles_eos30gr = eos30gr['smiles'].duplicated().sum()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of repeated smiles in eos2ta5: {repeated_smiles_eos2ta5}\")\n",
    "print(f\"Number of repeated smiles in eos4tcc: {repeated_smiles_eos4tcc}\")\n",
    "print(f\"Number of repeated smiles in eos30f3: {repeated_smiles_eos30f3}\")\n",
    "print(f\"Number of repeated smiles in eos30gr: {repeated_smiles_eos30gr}\")\n",
    "\n",
    "\n",
    "\n",
    "# Check repeated smiles between pairs of datasets\n",
    "repeated_smiles_eos2ta5_eos4tcc = pd.concat([eos2ta5['smiles'], eos4tcc['smiles']]).duplicated().sum()\n",
    "repeated_smiles_eos2ta5_eos30f3 = pd.concat([eos2ta5['smiles'], eos30f3['smiles']]).duplicated().sum()\n",
    "repeated_smiles_eos2ta5_eos30gr = pd.concat([eos2ta5['smiles'], eos30gr['smiles']]).duplicated().sum()\n",
    "repeated_smiles_eos4tcc_eos30f3 = pd.concat([eos4tcc['smiles'], eos30f3['smiles']]).duplicated().sum()\n",
    "repeated_smiles_eos4tcc_eos30gr = pd.concat([eos4tcc['smiles'], eos30gr['smiles']]).duplicated().sum()\n",
    "repeated_smiles_eos30f3_eos30gr = pd.concat([eos30f3['smiles'], eos30gr['smiles']]).duplicated().sum()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of repeated smiles between eos2ta5 and eos4tcc: {repeated_smiles_eos2ta5_eos4tcc}\")\n",
    "print(f\"Number of repeated smiles between eos2ta5 and eos30f3: {repeated_smiles_eos2ta5_eos30f3}\")\n",
    "print(f\"Number of repeated smiles between eos2ta5 and eos30gr: {repeated_smiles_eos2ta5_eos30gr}\")\n",
    "print(f\"Number of repeated smiles between eos4tcc and eos30f3: {repeated_smiles_eos4tcc_eos30f3}\")\n",
    "print(f\"Number of repeated smiles between eos4tcc and eos30gr: {repeated_smiles_eos4tcc_eos30gr}\")\n",
    "print(f\"Number of repeated smiles between eos30f3 and eos30gr: {repeated_smiles_eos30f3_eos30gr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build test dataset\n",
    "\n",
    "We collate in a single file the data from the NCATS repository and eliminate any duplicate molecules that exist in the training sets of the models already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(os.path.join(DATAPATH, \"test_data\", \"training_set_ncats.csv\"))\n",
    "df2 = pd.read_csv(os.path.join(DATAPATH, \"test_data\", \"validation_set_ncats.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['smiles', 'activity', 'source'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smiles eliminated:  0\n",
      "Smiles eliminated:  32\n"
     ]
    }
   ],
   "source": [
    "#merge and remove duplicates. Obtain InChiKeys for all\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "inchikeys = []\n",
    "for smi in df[\"smiles\"]:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        inchikey = Chem.MolToInchiKey(mol)\n",
    "    else:\n",
    "        inchikey = None\n",
    "    inchikeys += [inchikey]\n",
    "\n",
    "df[INCHICOL] = inchikeys\n",
    "total_len = len(df)\n",
    "df.dropna(subset=[INCHICOL], inplace=True)\n",
    "print(\"Smiles eliminated: \", total_len-len(df))\n",
    "total_len = len(df)\n",
    "df.drop_duplicates(subset=[SMICOL], inplace=True)\n",
    "print(\"Smiles eliminated: \", total_len-len(df))\n",
    "df = df[[SMICOL, INCHICOL, ACTCOL]]\n",
    "df.to_csv(os.path.join(DATAPATH, \"test_data\", \"ncats.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed inchikey: 0\n"
     ]
    }
   ],
   "source": [
    "# Now, from the all NCATS data, we eliminate duplicated molecules with training set\n",
    "## concatenate the training set together\n",
    "training_set = pd.concat([eos2ta5, eos4tcc, eos30f3, eos30gr], ignore_index=True)\n",
    "\n",
    "# Save the training set to a CSV file\n",
    "training_set.to_csv(os.path.join(DATAPATH, \"model_datasets\",\"training_set.csv\"), index=False)\n",
    "\n",
    "##Load the test dataset\n",
    "test_dataset= pd.read_csv(os.path.join(DATAPATH, \"test_data\", \"test_dataset.csv\")) \n",
    "\n",
    "# Calculate the number of removed InChiKey\n",
    "initial_inchikey_count = len(test_dataset)\n",
    "processed_test_dataset = test_dataset[~test_dataset['InChiKey'].isin(training_set['inchikey'])]\n",
    "removed_inchikey_count = initial_inchikey_count - len(processed_test_dataset)\n",
    "\n",
    "# Print the number of removed smiles\n",
    "print(f\"Number of removed inchikey: {removed_inchikey_count}\")\n",
    "\n",
    "# Save the processed test dataset to a file\n",
    "processed_test_dataset.to_csv(os.path.join(DATAPATH, \"test_data\", \"processed_test_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common inchikeys between training and test datasets: 0\n",
      "Common inchikey: set()\n"
     ]
    }
   ],
   "source": [
    "## Confirm that the training and test dataset has no Inchikey in common\n",
    "\n",
    "training_set = pd.read_csv(os.path.join(DATAPATH,\"model_datasets\", \"training_set.csv\"))\n",
    "test_set = pd.read_csv(os.path.join(DATAPATH,\"test_data\", \"processed_test_dataset.csv\"))\n",
    "\n",
    "# Check for common inchikeys\n",
    "common_inchikey = set(training_set['inchikey']).intersection(set(test_set['InChiKey']))\n",
    "\n",
    "# Print the number of common Inchikey\n",
    "print(f\"Number of common inchikeys between training and test datasets: {len(common_inchikey)}\")\n",
    "\n",
    "print(\"Common inchikey:\", common_inchikey)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
